{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d2e9106-4016-410d-b89d-cfb63253b070",
   "metadata": {},
   "source": [
    "# MAML-rnn - MCO two way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5939c859-e78d-4def-9fe8-0f9cf83ba2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     route       date  Passengers\n",
      "0  ABE_ATL 2022-01-31     1069.87\n",
      "1  ABE_ATL 2022-02-28      901.35\n",
      "2  ABE_ATL 2022-03-31     2046.49\n",
      "3  ABE_ATL 2022-04-30     1755.61\n",
      "4  ABE_ATL 2022-05-31     1507.64\n",
      "Date range in original data: 2022-01-31 00:00:00 to 2024-04-30 00:00:00\n",
      "Data Date Range: 2022-01-31 to 2024-04-30\n",
      "Date range in sequences data: 2023-01-31 to 2024-04-30\n",
      "Number of tasks: 841\n",
      "\n",
      "Meta-training Epoch 1/20\n",
      "Average Support Loss: 0.6726, Average Query Loss: 1.4561\n",
      "Meta-training Epoch 1 completed.\n",
      "\n",
      "Meta-training Epoch 2/20\n",
      "Average Support Loss: 0.6934, Average Query Loss: 1.3582\n",
      "Meta-training Epoch 2 completed.\n",
      "\n",
      "Meta-training Epoch 3/20\n",
      "Average Support Loss: 0.7321, Average Query Loss: 1.3066\n",
      "Meta-training Epoch 3 completed.\n",
      "\n",
      "Meta-training Epoch 4/20\n",
      "Average Support Loss: 0.8123, Average Query Loss: 1.2538\n",
      "Meta-training Epoch 4 completed.\n",
      "\n",
      "Meta-training Epoch 5/20\n",
      "Average Support Loss: 0.9416, Average Query Loss: 1.2155\n",
      "Meta-training Epoch 5 completed.\n",
      "\n",
      "Meta-training Epoch 6/20\n",
      "Average Support Loss: 1.0061, Average Query Loss: 1.1944\n",
      "Meta-training Epoch 6 completed.\n",
      "\n",
      "Meta-training Epoch 7/20\n",
      "Average Support Loss: 1.1489, Average Query Loss: 1.1408\n",
      "Meta-training Epoch 7 completed.\n",
      "\n",
      "Meta-training Epoch 8/20\n",
      "Average Support Loss: 1.1881, Average Query Loss: 1.1706\n",
      "Meta-training Epoch 8 completed.\n",
      "\n",
      "Meta-training Epoch 9/20\n",
      "Average Support Loss: 1.2447, Average Query Loss: 1.1220\n",
      "Meta-training Epoch 9 completed.\n",
      "\n",
      "Meta-training Epoch 10/20\n",
      "Average Support Loss: 1.2996, Average Query Loss: 1.1365\n",
      "Meta-training Epoch 10 completed.\n",
      "\n",
      "Meta-training Epoch 11/20\n",
      "Average Support Loss: 1.3705, Average Query Loss: 1.1515\n",
      "Meta-training Epoch 11 completed.\n",
      "\n",
      "Meta-training Epoch 12/20\n",
      "Average Support Loss: 1.4574, Average Query Loss: 1.2032\n",
      "Meta-training Epoch 12 completed.\n",
      "\n",
      "Meta-training Epoch 13/20\n",
      "Average Support Loss: 1.5095, Average Query Loss: 1.1331\n",
      "Meta-training Epoch 13 completed.\n",
      "\n",
      "Meta-training Epoch 14/20\n",
      "Average Support Loss: 1.5505, Average Query Loss: 1.1680\n",
      "Meta-training Epoch 14 completed.\n",
      "\n",
      "Meta-training Epoch 15/20\n",
      "Average Support Loss: 1.6950, Average Query Loss: 1.1692\n",
      "Meta-training Epoch 15 completed.\n",
      "\n",
      "Meta-training Epoch 16/20\n",
      "Average Support Loss: 1.6275, Average Query Loss: 1.3202\n",
      "Meta-training Epoch 16 completed.\n",
      "\n",
      "Meta-training Epoch 17/20\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, SimpleRNN, Dense, Dropout, Embedding, Concatenate, Flatten, RepeatVector\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import warnings\n",
    "import random\n",
    "import os\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 재현성을 위한 시드 설정\n",
    "seed_value = 42\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 환경 변수 설정\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "# 데이터베이스 연결 설정\n",
    "db_host = 'kaylee-db.cbgcswckszgl.us-east-1.rds.amazonaws.com'\n",
    "db_port = 3306\n",
    "db_user = 'lee'\n",
    "db_password = '1111'\n",
    "db_name = 'panel_reduced_reshaped_db'\n",
    "\n",
    "# SQLAlchemy 엔진 생성\n",
    "engine = create_engine(f'mysql+pymysql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}')\n",
    "\n",
    "# 데이터베이스에서 데이터 로드 (특정 허브 공항 MCO 중심) -- Twoway\n",
    "with engine.connect() as connection:\n",
    "    query = text(\"\"\"\n",
    "    SELECT route, date, Passengers\n",
    "    FROM panel_reduced_reshaped\n",
    "    WHERE `Destination Airport` = 'ATL' \n",
    "       OR `Origin Airport` = 'ATL' \n",
    "    ORDER BY route, date\n",
    "    \"\"\")\n",
    "    df = pd.read_sql(query, connection, parse_dates=['date'])  # 'date' 컬럼을 datetime으로 파싱\n",
    "    print(df.head())\n",
    "    \n",
    "# 날짜 범위 확인\n",
    "print(f\"Date range in original data: {df['date'].min()} to {df['date'].max()}\")\n",
    "\n",
    "# 'route' 컬럼 정리\n",
    "df['route'] = df['route'].str.strip().str.upper()  # 공백 제거 및 대문자 변환\n",
    "\n",
    "# 노선 선택\n",
    "num_routes = min(2000, df['route'].nunique()) ################## 전체 왕복 선택\n",
    "unique_routes = df['route'].unique()[:num_routes]\n",
    "df = df[df['route'].isin(unique_routes)].reset_index(drop=True)\n",
    "\n",
    "# 노선 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "df['Route_ID'] = label_encoder.fit_transform(df['route'])\n",
    "num_routes = len(label_encoder.classes_)\n",
    "\n",
    "# 시퀀스 길이 설정\n",
    "sequence_length = 12  # 12개월 시퀀스\n",
    "\n",
    "# 데이터의 최소 및 최대 날짜 확인\n",
    "min_date = df['date'].min()\n",
    "max_date = df['date'].max()\n",
    "print(f\"Data Date Range: {min_date.date()} to {max_date.date()}\")\n",
    "\n",
    "# 시퀀스 생성\n",
    "def generate_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    for route in data['route'].unique():\n",
    "        route_data = data[data['route'] == route].sort_values('date')\n",
    "        passenger_counts = route_data['Passengers'].values\n",
    "        dates_list = route_data['date'].values\n",
    "        route_id = route_data['Route_ID'].iloc[0]\n",
    "\n",
    "        if len(passenger_counts) < seq_length + 1:\n",
    "            continue\n",
    "\n",
    "        for i in range(len(passenger_counts) - seq_length):\n",
    "            seq = passenger_counts[i:i + seq_length]\n",
    "            target = passenger_counts[i + seq_length]\n",
    "            target_date = dates_list[i + seq_length]\n",
    "\n",
    "            sequences.append({\n",
    "                'Sequence': seq,\n",
    "                'Target': target,\n",
    "                'Route_ID': route_id,\n",
    "                'Date': target_date,\n",
    "                'Route': route\n",
    "            })\n",
    "    return sequences\n",
    "\n",
    "sequences = generate_sequences(df, sequence_length)\n",
    "data = pd.DataFrame(sequences)\n",
    "\n",
    "# 날짜 파싱 확인\n",
    "print(f\"Date range in sequences data: {data['Date'].min().date()} to {data['Date'].max().date()}\")\n",
    "\n",
    "# 데이터 정렬\n",
    "data = data.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "# 작업 생성 및 분할\n",
    "def create_fomaml_tasks(data, support_size, query_size):\n",
    "    tasks = []\n",
    "    for route in data['Route'].unique():\n",
    "        route_data = data[data['Route'] == route].sort_values('Date').reset_index(drop=True)\n",
    "        total_samples = len(route_data)\n",
    "        if total_samples < support_size + query_size:\n",
    "            continue\n",
    "\n",
    "        # 시간 기반 분할\n",
    "        support_set = route_data.iloc[:support_size]\n",
    "        query_set = route_data.iloc[support_size:support_size + query_size]\n",
    "\n",
    "        if len(support_set) >= 1 and len(query_set) >= 1:\n",
    "            tasks.append({\n",
    "                'route': route,\n",
    "                'support_set': support_set,\n",
    "                'query_set': query_set\n",
    "            })\n",
    "    return tasks\n",
    "\n",
    "support_size = 10\n",
    "query_size = 6  # 마지막 6개월\n",
    "tasks = create_fomaml_tasks(data, support_size, query_size)\n",
    "print(f\"Number of tasks: {len(tasks)}\")\n",
    "\n",
    "# 모든 작업을 메타-훈련과 메타-테스트에 사용\n",
    "meta_train_tasks = tasks\n",
    "meta_test_tasks = tasks\n",
    "\n",
    "# 작업 스케일링\n",
    "def scale_task(task):\n",
    "    X_support = np.array([seq for seq in task['support_set']['Sequence']])\n",
    "    y_support = task['support_set']['Target'].values.reshape(-1, 1)\n",
    "    X_query = np.array([seq for seq in task['query_set']['Sequence']])\n",
    "    y_query = task['query_set']['Target'].values.reshape(-1, 1)\n",
    "\n",
    "    # 지원 세트로 스케일러 피팅\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    scaler_X.fit(X_support.reshape(-1, 1))\n",
    "    scaler_y.fit(y_support)\n",
    "\n",
    "    # 지원 세트와 쿼리 세트 변환\n",
    "    X_support_scaled = scaler_X.transform(X_support.reshape(-1, 1)).reshape(-1, sequence_length, 1)\n",
    "    y_support_scaled = scaler_y.transform(y_support)\n",
    "    X_query_scaled = scaler_X.transform(X_query.reshape(-1, 1)).reshape(-1, sequence_length, 1)\n",
    "    y_query_scaled = scaler_y.transform(y_query)\n",
    "\n",
    "    task['X_support'] = X_support_scaled\n",
    "    task['y_support'] = y_support_scaled\n",
    "    task['X_query'] = X_query_scaled\n",
    "    task['y_query'] = y_query_scaled\n",
    "    task['scaler_X'] = scaler_X  # 역변환을 위한 스케일러 저장\n",
    "    task['scaler_y'] = scaler_y\n",
    "    return task\n",
    "\n",
    "meta_train_tasks = [scale_task(task) for task in meta_train_tasks]\n",
    "meta_test_tasks = meta_train_tasks  # 동일한 작업 사용\n",
    "\n",
    "# 모델 정의 - RNN\n",
    "def create_model():\n",
    "    embedding_dim = int(np.sqrt(num_routes))\n",
    "\n",
    "    passenger_input = Input(shape=(sequence_length, 1), name='Passenger_Input')\n",
    "    route_input = Input(shape=(1,), name='Route_Input')\n",
    "\n",
    "    route_embedding = Embedding(input_dim=num_routes, output_dim=embedding_dim, input_length=1)(route_input)\n",
    "    route_embedding = Flatten()(route_embedding)\n",
    "    route_embedding_repeated = RepeatVector(sequence_length)(route_embedding)\n",
    "\n",
    "    combined_input = Concatenate(axis=2)([passenger_input, route_embedding_repeated])\n",
    "\n",
    "    # RNN 레이어 정의\n",
    "    rnn_out = SimpleRNN(32, activation='tanh', kernel_regularizer=l2(0.01))(combined_input)\n",
    "    rnn_out = Dropout(0.3)(rnn_out)  # 드롭아웃 비율 조정\n",
    "\n",
    "    output = Dense(1, activation='linear')(rnn_out)\n",
    "\n",
    "    model = Model(inputs=[passenger_input, route_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "meta_model = create_model()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "meta_epochs = 20  # 에포크 수 설정 20으로 증가\n",
    "adaptation_steps = min(5, support_size)\n",
    "adaptation_learning_rate = 1e-2 ################# 1e-2 에서 2e-2으로 일시 변경\n",
    "meta_learning_rate = 1e-3 ###################1e-3 에서 2e-3 로 일시 변경\n",
    "\n",
    "meta_optimizer = tf.keras.optimizers.Adam(learning_rate=meta_learning_rate)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# 메타-훈련 루프\n",
    "best_query_loss = float('inf')  # 최소 Query Loss를 추적하기 위한 초기 값\n",
    "best_weights = None  # 최적 파라미터를 저장할 변수\n",
    "\n",
    "for epoch in range(meta_epochs):\n",
    "    print(f\"\\nMeta-training Epoch {epoch + 1}/{meta_epochs}\")\n",
    "    epoch_support_loss = 0\n",
    "    epoch_query_loss = 0\n",
    "    num_tasks = len(meta_train_tasks)\n",
    "\n",
    "    for task in meta_train_tasks:\n",
    "        # 데이터 가져오기\n",
    "        X_support = task['X_support']\n",
    "        y_support = task['y_support']\n",
    "        X_query = task['X_query']\n",
    "        y_query = task['y_query']\n",
    "        route_id = task['support_set']['Route_ID'].iloc[0]\n",
    "\n",
    "        # 내부 루프\n",
    "        with tf.GradientTape() as tape:\n",
    "            # 적응된 모델\n",
    "            adapted_model = create_model()\n",
    "            adapted_model.set_weights(meta_model.get_weights())\n",
    "\n",
    "            # 적응 단계용 옵티마이저 생성\n",
    "            adaptation_optimizer = tf.keras.optimizers.SGD(learning_rate=adaptation_learning_rate)\n",
    "\n",
    "            # 적응 단계\n",
    "            for _ in range(adaptation_steps):\n",
    "                with tf.GradientTape() as inner_tape:\n",
    "                    preds_support = adapted_model([X_support, np.full((len(X_support), 1), route_id)], training=True)\n",
    "                    support_loss = loss_fn(y_support, preds_support)\n",
    "                grads = inner_tape.gradient(support_loss, adapted_model.trainable_variables)\n",
    "                adaptation_optimizer.apply_gradients(zip(grads, adapted_model.trainable_variables))\n",
    "\n",
    "            # 쿼리 세트에서 손실 계산\n",
    "            preds_query = adapted_model([X_query, np.full((len(X_query), 1), route_id)], training=False)\n",
    "            query_loss = loss_fn(y_query, preds_query)\n",
    "\n",
    "        # 손실 누적\n",
    "        epoch_support_loss += support_loss.numpy()\n",
    "        epoch_query_loss += query_loss.numpy()\n",
    "\n",
    "        # 쿼리 손실에 대한 그래디언트 계산\n",
    "        grads = tape.gradient(query_loss, adapted_model.trainable_variables)\n",
    "        # 메타-모델에 그래디언트 적용\n",
    "        meta_optimizer.apply_gradients(zip(grads, meta_model.trainable_variables))\n",
    "\n",
    "    # 에포크별 평균 손실 계산\n",
    "    avg_support_loss = epoch_support_loss / num_tasks\n",
    "    avg_query_loss = epoch_query_loss / num_tasks\n",
    "    print(f\"Average Support Loss: {avg_support_loss:.4f}, Average Query Loss: {avg_query_loss:.4f}\")\n",
    "    print(f\"Meta-training Epoch {epoch + 1} completed.\")\n",
    "\n",
    "    # 최적의 Query Loss에서 가중치 저장\n",
    "    if avg_query_loss < best_query_loss:\n",
    "        best_query_loss = avg_query_loss\n",
    "        best_weights = meta_model.get_weights()  # 최적 가중치 저장\n",
    "\n",
    "# 메타-훈련 종료 후 최적 가중치 설정\n",
    "meta_model.set_weights(best_weights)\n",
    "print(\"Meta-training completed. Using the weights with the lowest query loss.\")\n",
    "\n",
    "# 메타-테스트 (파인튜닝 및 평가)\n",
    "performance_metrics = []\n",
    "adapted_models = {}  # 노선별로 파인튜닝된 모델 저장\n",
    "\n",
    "# 수정된 코드: 노선 이름순으로 정렬하여 성능 계산\n",
    "meta_test_tasks_sorted = sorted(meta_test_tasks, key=lambda x: x['route'])\n",
    "\n",
    "# 성능 지표 계산 (메타-테스트의 파인튜닝 및 평가 부분에 해당)\n",
    "for task in meta_test_tasks_sorted:\n",
    "    route = task['route']\n",
    "    print(f\"\\nFine-tuning on route: {route}\")\n",
    "    # 데이터 가져오기\n",
    "    X_support = task['X_support']\n",
    "    y_support = task['y_support']\n",
    "    X_query = task['X_query']\n",
    "    y_query = task['y_query']\n",
    "    route_id = task['support_set']['Route_ID'].iloc[0]\n",
    "    route_id_tensor = tf.constant([[route_id]], dtype=tf.int32)\n",
    "\n",
    "    scaler_y = task['scaler_y']  # 노선별 스케일러 사용\n",
    "\n",
    "    # 메타-모델 복제\n",
    "    adapted_model = create_model()\n",
    "    adapted_model.set_weights(meta_model.get_weights())\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=adaptation_learning_rate)\n",
    "\n",
    "    # 지원 세트로 파인튜닝\n",
    "    for _ in range(adaptation_steps):\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = adapted_model([X_support, np.repeat(route_id_tensor, len(X_support), axis=0)], training=True)\n",
    "            loss = loss_fn(y_support, preds)\n",
    "        grads = tape.gradient(loss, adapted_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, adapted_model.trainable_variables))\n",
    "\n",
    "    # 쿼리 세트에서 예측\n",
    "    query_preds_scaled = adapted_model([X_query, np.repeat(route_id_tensor, len(X_query), axis=0)], training=False)\n",
    "    y_pred = scaler_y.inverse_transform(query_preds_scaled.numpy()).flatten()\n",
    "    y_actual = task['query_set']['Target'].values.flatten()\n",
    "\n",
    "    # 성능 지표 계산\n",
    "    mse = mean_squared_error(y_actual, y_pred)\n",
    "    mae = mean_absolute_error(y_actual, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # MAPE 계산 (0으로 나누기 방지)\n",
    "    non_zero_indices = y_actual != 0\n",
    "    if np.any(non_zero_indices):\n",
    "        mape = np.mean(np.abs((y_actual[non_zero_indices] - y_pred[non_zero_indices]) / y_actual[non_zero_indices])) * 100\n",
    "    else:\n",
    "        mape = np.nan  # 실제 값이 모두 0인 경우 정의되지 않음\n",
    "\n",
    "    # SMAPE 계산\n",
    "    smape = np.mean(2 * np.abs(y_actual - y_pred) / (np.abs(y_actual) + np.abs(y_pred))) * 100\n",
    "\n",
    "    print(f\"Performance on route {route}: MSE={mse:.2f}, MAE={mae:.2f}, RMSE={rmse:.2f}, MAPE={mape:.2f}%, SMAPE={smape:.2f}%\")\n",
    "\n",
    "    # 성능 지표 수집\n",
    "    performance_metrics.append({\n",
    "        'Route': route,\n",
    "        'MSE': mse,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'MAPE': mape,\n",
    "        'SMAPE': smape\n",
    "    })\n",
    "\n",
    "    # 노선별로 파인튜닝된 모델 저장\n",
    "    adapted_models[route] = adapted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839ab7f2-06c3-4df0-82d2-ba2931b679b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로컬 모델 학습 및 성능 비교\n",
    "local_performance_metrics = []\n",
    "local_models = {}  # 로컬 모델을 저장할 딕셔너리 선언\n",
    "\n",
    "# 노선 이름순으로 meta_test_tasks 정렬\n",
    "meta_test_tasks_sorted = sorted(meta_test_tasks, key=lambda x: x['route'])\n",
    "\n",
    "for task in meta_test_tasks_sorted:\n",
    "    route = task['route']\n",
    "    print(f\"\\nTraining local model for route: {route}\")\n",
    "\n",
    "    # 데이터 가져오기\n",
    "    X_support = task['X_support']\n",
    "    y_support = task['y_support']\n",
    "    X_query = task['X_query']\n",
    "    y_query = task['y_query']\n",
    "    route_id = task['support_set']['Route_ID'].iloc[0]\n",
    "    route_id_tensor = tf.constant([[route_id]], dtype=tf.int32)\n",
    "\n",
    "    scaler_y = task['scaler_y']  # 노선별 스케일러 사용\n",
    "\n",
    "    # 로컬 모델 생성\n",
    "    local_model = create_model()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=meta_learning_rate)\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    # 로컬 모델 학습\n",
    "    local_model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "    local_model.fit([X_support, np.repeat(route_id_tensor, len(X_support), axis=0)], y_support,\n",
    "                    epochs=meta_epochs, verbose=0)\n",
    "\n",
    "    # 쿼리 세트에서 예측\n",
    "    query_preds_scaled = local_model([X_query, np.repeat(route_id_tensor, len(X_query), axis=0)], training=False)\n",
    "    y_pred = scaler_y.inverse_transform(query_preds_scaled.numpy()).flatten()\n",
    "    y_actual = task['query_set']['Target'].values.flatten()\n",
    "\n",
    "    # 성능 지표 계산\n",
    "    mse = mean_squared_error(y_actual, y_pred)\n",
    "    mae = mean_absolute_error(y_actual, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    # MAPE 계산\n",
    "    non_zero_indices = y_actual != 0\n",
    "    if np.any(non_zero_indices):\n",
    "        mape = np.mean(np.abs((y_actual[non_zero_indices] - y_pred[non_zero_indices]) / y_actual[non_zero_indices])) * 100\n",
    "    else:\n",
    "        mape = np.nan\n",
    "\n",
    "    # SMAPE 계산\n",
    "    smape = np.mean(2 * np.abs(y_actual - y_pred) / (np.abs(y_actual) + np.abs(y_pred))) * 100\n",
    "\n",
    "    print(f\"Local model performance on route {route}: MSE={mse:.2f}, MAE={mae:.2f}, RMSE={rmse:.2f}, MAPE={mape:.2f}%, SMAPE={smape:.2f}%\")\n",
    "\n",
    "    # 성능 지표 수집\n",
    "    local_performance_metrics.append({\n",
    "        'Route': route,\n",
    "        'MSE': mse,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'MAPE': mape,\n",
    "        'SMAPE': smape\n",
    "    })\n",
    "    # 로컬 모델 저장 (여기에 추가)\n",
    "    local_models[route] = local_model  # 각 노선의 로컬 모델을 딕셔너리에 저장\n",
    "\n",
    "# 성능 비교\n",
    "comparison_results = []\n",
    "\n",
    "# 성능 지표 리스트를 노선 이름순으로 정렬\n",
    "performance_metrics_sorted = sorted(performance_metrics, key=lambda x: x['Route'])\n",
    "local_performance_metrics_sorted = sorted(local_performance_metrics, key=lambda x: x['Route'])\n",
    "\n",
    "for meta_metric, local_metric in zip(performance_metrics_sorted, local_performance_metrics_sorted):\n",
    "    route = meta_metric['Route']\n",
    "    meta_mse = meta_metric['MSE']\n",
    "    meta_mae = meta_metric['MAE']\n",
    "    meta_rmse = meta_metric['RMSE']\n",
    "    meta_mape = meta_metric['MAPE']\n",
    "    meta_smape = meta_metric['SMAPE']\n",
    "\n",
    "    local_mse = local_metric['MSE']\n",
    "    local_mae = local_metric['MAE']\n",
    "    local_rmse = local_metric['RMSE']\n",
    "    local_mape = local_metric['MAPE']\n",
    "    local_smape = local_metric['SMAPE']\n",
    "\n",
    "    # 개선율 계산\n",
    "    improvement_mse = (local_mse - meta_mse) / local_mse * 100\n",
    "    improvement_mae = (local_mae - meta_mae) / local_mae * 100\n",
    "    improvement_rmse = (local_rmse - meta_rmse) / local_rmse * 100\n",
    "    improvement_mape = (local_mape - meta_mape) / local_mape * 100 if local_mape and meta_mape else np.nan\n",
    "    improvement_smape = (local_smape - meta_smape) / local_smape * 100 if local_smape and meta_smape else np.nan\n",
    "\n",
    "    comparison_results.append({\n",
    "        'Route': route,\n",
    "        'Meta MSE': meta_mse,\n",
    "        'Local MSE': local_mse,\n",
    "        'MSE Improvement (%)': improvement_mse,\n",
    "        'Meta MAE': meta_mae,\n",
    "        'Local MAE': local_mae,\n",
    "        'MAE Improvement (%)': improvement_mae,\n",
    "        'Meta RMSE': meta_rmse,\n",
    "        'Local RMSE': local_rmse,\n",
    "        'RMSE Improvement (%)': improvement_rmse,\n",
    "        'Meta MAPE': meta_mape,\n",
    "        'Local MAPE': local_mape,\n",
    "        'MAPE Improvement (%)': improvement_mape,\n",
    "        'Meta SMAPE': meta_smape,\n",
    "        'Local SMAPE': local_smape,\n",
    "        'SMAPE Improvement (%)': improvement_smape\n",
    "    })\n",
    "\n",
    "# 결과를 데이터프레임으로 표시\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "print(comparison_df)\n",
    "comparison_df.to_csv('Comparison_Meta_Tuning_vs_Local.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4a6d69-b643-4dc4-82ca-80ed0f7413fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel, wilcoxon\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# DataFrame에서 필요한 열 추출\n",
    "mse_improvements = comparison_df['MSE Improvement (%)'].values\n",
    "mae_improvements = comparison_df['MAE Improvement (%)'].values\n",
    "rmse_improvements = comparison_df['RMSE Improvement (%)'].values\n",
    "mape_improvements = comparison_df['MAPE Improvement (%)'].values\n",
    "smape_improvements = comparison_df['SMAPE Improvement (%)'].values\n",
    "\n",
    "# 1. 평균 향상도와 향상 비율 분석\n",
    "mean_mse_improvement = np.mean(mse_improvements)\n",
    "mean_mae_improvement = np.mean(mae_improvements)\n",
    "mean_rmse_improvement = np.mean(rmse_improvements)\n",
    "mean_mape_improvement = np.nanmean(mape_improvements)  # MAPE는 NaN 처리 필요\n",
    "mean_smape_improvement = np.nanmean(smape_improvements)  # SMAPE는 NaN 처리 필요\n",
    "\n",
    "mse_improved_ratio = np.sum(mse_improvements > 0) / len(mse_improvements) * 100\n",
    "mae_improved_ratio = np.sum(mae_improvements > 0) / len(mae_improvements) * 100\n",
    "rmse_improved_ratio = np.sum(rmse_improvements > 0) / len(rmse_improvements) * 100\n",
    "mape_improved_ratio = np.sum(~np.isnan(mape_improvements) & (mape_improvements > 0)) / np.sum(~np.isnan(mape_improvements)) * 100\n",
    "smape_improved_ratio = np.sum(~np.isnan(smape_improvements) & (smape_improvements > 0)) / np.sum(~np.isnan(smape_improvements)) * 100\n",
    "\n",
    "print(\"1. Average Improvement:\")\n",
    "print(f\"Average MSE Improvement: {mean_mse_improvement:.2f}%\")\n",
    "print(f\"Average MAE Improvement: {mean_mae_improvement:.2f}%\")\n",
    "print(f\"Average RMSE Improvement: {mean_rmse_improvement:.2f}%\")\n",
    "print(f\"Average MAPE Improvement: {mean_mape_improvement:.2f}%\")\n",
    "print(f\"Average SMAPE Improvement: {mean_smape_improvement:.2f}%\\n\")\n",
    "\n",
    "print(\"1. Improvement Ratios:\")\n",
    "print(f\"MSE Improved in {mse_improved_ratio:.2f}% of routes\")\n",
    "print(f\"MAE Improved in {mae_improved_ratio:.2f}% of routes\")\n",
    "print(f\"RMSE Improved in {rmse_improved_ratio:.2f}% of routes\")\n",
    "print(f\"MAPE Improved in {mape_improved_ratio:.2f}% of routes\")\n",
    "print(f\"SMAPE Improved in {smape_improved_ratio:.2f}% of routes\\n\")\n",
    "\n",
    "# 2. 통계적 검정 (유의미한 차이 확인)\n",
    "# 쌍체 t-검정 (정규성 가정)\n",
    "t_stat_mse, p_value_mse = ttest_rel(comparison_df['Meta MSE'], comparison_df['Local MSE'])\n",
    "t_stat_mae, p_value_mae = ttest_rel(comparison_df['Meta MAE'], comparison_df['Local MAE'])\n",
    "t_stat_rmse, p_value_rmse = ttest_rel(comparison_df['Meta RMSE'], comparison_df['Local RMSE'])\n",
    "\n",
    "# 윌콕슨 검정 (비모수)\n",
    "_, p_value_mape = wilcoxon(comparison_df['Meta MAPE'].dropna(), comparison_df['Local MAPE'].dropna())\n",
    "_, p_value_smape = wilcoxon(comparison_df['Meta SMAPE'].dropna(), comparison_df['Local SMAPE'].dropna())\n",
    "\n",
    "print(\"2. Statistical Significance Testing Results (p-values):\")\n",
    "print(f\"MSE Improvement p-value: {p_value_mse:.4f}\")\n",
    "print(f\"MAE Improvement p-value: {p_value_mae:.4f}\")\n",
    "print(f\"RMSE Improvement p-value: {p_value_rmse:.4f}\")\n",
    "print(f\"MAPE Improvement p-value: {p_value_mape:.4f}\")\n",
    "print(f\"SMAPE Improvement p-value: {p_value_smape:.4f}\\n\")\n",
    "\n",
    "# 3. 모든 메트릭에서 일관성 확인\n",
    "# 일관되게 향상된 비율 확인 (각 항목이 모두 양수인 경우만 카운트)\n",
    "consistent_improvement_count = np.sum(\n",
    "    (mse_improvements > 0) & \n",
    "    (mae_improvements > 0) & \n",
    "    (rmse_improvements > 0) & \n",
    "    (mape_improvements > 0) & \n",
    "    (smape_improvements > 0)\n",
    ")\n",
    "consistent_improvement_ratio = consistent_improvement_count / len(mse_improvements) * 100\n",
    "\n",
    "print(\"3. Consistent Improvement Check:\")\n",
    "print(f\"Routes with consistent improvement across all metrics: {consistent_improvement_ratio:.2f}%\\n\")\n",
    "\n",
    "# 결론\n",
    "if consistent_improvement_ratio > 50 and p_value_mse < 0.05 and p_value_mae < 0.05:\n",
    "    print(\"Meta+Tuning model shows significant and consistent improvements over the Local model.\")\n",
    "else:\n",
    "    print(\"Improvements are not consistently significant across all metrics.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4c15e5-731a-44e2-82ce-05db166d1935",
   "metadata": {},
   "source": [
    "# Visualization of predictions for each route in meta-test tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9995c633-761f-4569-a6f9-917be3ea91d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new\n",
    "# Route mapping from Route_ID to route name\n",
    "route_mapping = dict(zip(df['Route_ID'], df['route']))\n",
    "\n",
    "# 데이터 순서상 앞의 10개 노선 선택\n",
    "unique_routes_ordered = df['route'].unique()\n",
    "top_routes = unique_routes_ordered[:10].tolist()\n",
    "print(f\"First 10 routes in data order: {top_routes}\")\n",
    "\n",
    "# 첫 10개 노선에 해당하는 meta_test_tasks 필터링\n",
    "meta_test_tasks_top10 = [task for task in meta_test_tasks if task['route'] in top_routes]\n",
    "\n",
    "# 첫 10개 노선을 노선 이름에 따라 정렬\n",
    "meta_test_tasks_sorted = sorted(meta_test_tasks_top10, key=lambda x: x['route'])\n",
    "\n",
    "# For each route in the top 10 routes\n",
    "for task in meta_test_tasks_sorted:\n",
    "    route = task['route']\n",
    "    route_id = task['support_set']['Route_ID'].iloc[0]\n",
    "    route_name = route_mapping[route_id]\n",
    "    print(f\"\\nEvaluating and plotting results for route: {route_name}\")\n",
    "\n",
    "    # Get the adapted model for the current route\n",
    "    adapted_model = adapted_models[route]  # 메타모델 + 파인튜닝 모델\n",
    "    local_model = local_models[route]      # 로컬 모델\n",
    "\n",
    "    # Get full data for the route from the original DataFrame\n",
    "    route_data_full = df[df['route'] == route].sort_values('date')\n",
    "\n",
    "    # Get dates and passenger counts for full data\n",
    "    dates_full = pd.to_datetime(route_data_full['date']).reset_index(drop=True)\n",
    "    passengers_full = route_data_full['Passengers'].values.flatten()\n",
    "\n",
    "    # Get support and query sets\n",
    "    support_set = task['support_set']\n",
    "    query_set = task['query_set']\n",
    "\n",
    "    # Get dates and passenger counts for support set\n",
    "    dates_support = pd.to_datetime(support_set['Date']).reset_index(drop=True)\n",
    "    passengers_support = support_set['Target'].values.flatten()\n",
    "\n",
    "    # Get dates and passenger counts for query set (actual)\n",
    "    dates_query = pd.to_datetime(query_set['Date']).reset_index(drop=True)\n",
    "    passengers_query_actual = query_set['Target'].values.flatten()\n",
    "\n",
    "    # Get predictions on query set from adapted model\n",
    "    y_pred_scaled_adapted = adapted_model([task['X_query'], np.full((len(task['X_query']), 1), route_id)], training=False)\n",
    "    passengers_query_pred_adapted = task['scaler_y'].inverse_transform(y_pred_scaled_adapted.numpy()).flatten()\n",
    "\n",
    "    # Get predictions on query set from local model\n",
    "    y_pred_scaled_local = local_model([task['X_query'], np.full((len(task['X_query']), 1), route_id)], training=False)\n",
    "    passengers_query_pred_local = task['scaler_y'].inverse_transform(y_pred_scaled_local.numpy()).flatten()\n",
    "\n",
    "    # Retrieve performance metrics for the current route\n",
    "    metrics_adapted = next(item for item in performance_metrics if item['Route'] == route)\n",
    "    mse_adapted = metrics_adapted['MSE']\n",
    "    mae_adapted = metrics_adapted['MAE']\n",
    "    \n",
    "    rmse_adapted = metrics_adapted['RMSE']\n",
    "    mape_adapted = metrics_adapted['MAPE']\n",
    "    smape_adapted = metrics_adapted['SMAPE']\n",
    "\n",
    "    metrics_local = next(item for item in local_performance_metrics if item['Route'] == route)\n",
    "    mse_local = metrics_local['MSE']\n",
    "    mae_local = metrics_local['MAE']\n",
    "\n",
    "    rmse_local = metrics_local['RMSE']\n",
    "    mape_local = metrics_local['MAPE']\n",
    "    smape_local = metrics_local['SMAPE']\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "    # Plot full actual data\n",
    "    plt.plot(dates_full, passengers_full, label='Actual Data', color='blue', marker='.', linestyle='-')\n",
    "\n",
    "    # Plot support set (training data)\n",
    "    plt.plot(dates_support, passengers_support, label='Training Data', color='blue', marker='o', linestyle='-')\n",
    "\n",
    "    # Plot actual query set data\n",
    "    plt.plot(dates_query, passengers_query_actual, label='Test Actual', color='green', marker='s', linestyle='-')\n",
    "\n",
    "    # Plot predicted query set data from adapted model\n",
    "    plt.plot(dates_query, passengers_query_pred_adapted, label='Meta+Tuning Predicted', color='red', linestyle='--', marker='D')\n",
    "\n",
    "    # Plot predicted query set data from local model\n",
    "    plt.plot(dates_query, passengers_query_pred_local, label='Local Model Predicted', color='orange', linestyle='--', marker='X')\n",
    "\n",
    "    # Add connection between Actual (blue) and Test Actual (green)\n",
    "    plt.plot([dates_support.iloc[-1], dates_query.iloc[0]], \n",
    "             [passengers_support[-1], passengers_query_actual[0]], \n",
    "             color='green', linestyle='-')\n",
    "\n",
    "    # Add connections from Actual (blue) to Meta+Tuning Predicted (red) and Local Model Predicted (orange)\n",
    "    plt.plot([dates_support.iloc[-1], dates_query.iloc[0]], \n",
    "             [passengers_support[-1], passengers_query_pred_adapted[0]], \n",
    "             color='red', linestyle='--')\n",
    "    plt.plot([dates_support.iloc[-1], dates_query.iloc[0]], \n",
    "             [passengers_support[-1], passengers_query_pred_local[0]], \n",
    "             color='orange', linestyle='--')\n",
    "\n",
    "    # Formatting the x-axis\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    plt.gcf().autofmt_xdate()\n",
    "\n",
    "    # Set x-axis limits to cover the full date range\n",
    "    plt.xlim([dates_full.min(), dates_full.max()])\n",
    "\n",
    "    # Title and labels (including performance metrics)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Monthly Passengers')\n",
    "    plt.title(f'Actual vs Predicted Passengers for Route {route_name}\\n'\n",
    "              f'MAML+Tuning MSE: {mse_adapted:.2f}, MAE: {mae_adapted:.2f}, RMSE: {rmse_adapted:.2f}, MAPE: {mape_adapted:.2f}%, SMAPE: {smape_adapted:.2f}%\\n'\n",
    "              f'Local Model MSE: {mse_local:.2f}, MAE: {mae_local:.2f}, RMSE: {rmse_local:.2f}, MAPE: {mape_local:.2f}%, SMAPE: {smape_local:.2f}%')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "\n",
    "    # Dynamic Y-Axis Scaling\n",
    "    all_passengers = np.concatenate([passengers_full, passengers_query_pred_adapted, passengers_query_pred_local])\n",
    "    y_min = np.min(all_passengers)\n",
    "    y_max = np.max(all_passengers)\n",
    "    padding = (y_max - y_min) * 0.1  # 10% padding\n",
    "    plt.ylim(y_min - padding, y_max + padding)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    #저장 1회만 할것 ####\n",
    "    plt.savefig(f\"route_{route_name}_comparison.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1394283-99ba-4fa7-93c2-2e2ff378390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cafbbf-0fe9-43e6-87fa-1932ac1fe086",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df[\"SMAPE Improvement (%)\"].plot.hist(bins=100,edgecolor='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0214d637-61e8-4fbe-b07a-9989429fcbd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Global Forecasting Env",
   "language": "python",
   "name": "global_forecasting_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
