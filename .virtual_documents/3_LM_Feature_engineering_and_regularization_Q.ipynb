








pip install pymysql


import pandas as pd
import numpy as np



fish_df= pd.read_csv("https://raw.githubusercontent.com/kjmobile/data/refs/heads/main/ml/fish.csv")


fish_df


fish = fish_df.to_numpy()
fish[:5,:]


# We will use weight as target y,  and  length, Height, Width as X.
from sklearn.model_selection import train_test_split
train_X, test_X, train_y, test_y = train_test_split(fish[:,1:], fish[:,0], random_state=42)


train_X.shape


train_y





from sklearn.preprocessing import PolynomialFeatures


poly = PolynomialFeatures(degree = 2, include_bias=False) # degree=2 is default

poly.fit(train_X)
train_poly = poly.transform(train_X)


print(train_poly.shape)


train_poly[:5]


np.set_printoptions(suppress=True) # suppress scientific notation
train_poly[:5]


poly.get_feature_names_out()


#Now, transform the test set using the same poly class used for train set.
test_poly = poly.transform(test_X)


test_poly[:2]





from sklearn.linear_model import LinearRegression

m0 = LinearRegression()
m0.fit(train_poly, train_y)
print(m0.score(train_poly, train_y))


print(m0.score(test_poly, test_y))






poly_5 = PolynomialFeatures(degree = 5, include_bias=False)

poly_5.fit(train_X)
train_poly_5 = poly_5.transform(train_X)
test_poly_5 = poly_5.transform(test_X)


m1=LinearRegression()
m1.fit(train_poly_5, train_y)



test_poly_5.shape


print(m1.score(train_poly_5, train_y))
print(m1.score(test_poly_5, test_y))
# R-squared, -120.8 ? ; Something is wrong as the model seriously "overfitted" with train set.


























from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_poly_5)

train_scaled = ss.transform(train_poly_5)
test_scaled = ss.transform(test_poly_5)





from sklearn.linear_model import Lasso

lasso = Lasso()
lasso.fit(train_scaled, train_y)
print(lasso.score(train_scaled, train_y))


print(lasso.score(test_scaled, test_y))





train_score_l = []
test_score_l = []

alpha_list = [0.001, 0.01, 0.1, 1, 10,100]
for alpha in alpha_list:
    # lasso model
    lasso = Lasso(alpha=alpha, max_iter=10000, tol=0.01)
    # train lasso model
    lasso.fit(train_scaled, train_y)
    # Save R-squares from train and test
    train_score_l.append(lasso.score(train_scaled, train_y))
    test_score_l.append(lasso.score(test_scaled, test_y))


test_score_l


import matplotlib.pyplot as plt

plt.plot(np.log10(alpha_list), train_score_l, label='Train R2 Score')
plt.plot(np.log10(alpha_list), test_score_l, color='red', label ='Test R2 Score')
plt.vlines(x=1, ymin=0.92, ymax=0.995, ls=':', color='k')
plt.xlabel('alpha')
plt.ylabel('$R^2$', rotation=0)
plt.legend()
plt.show()


lasso = Lasso(alpha=10)
lasso.fit(train_scaled, train_y)

print(lasso.score(train_scaled, train_y))
print(lasso.score(test_scaled, test_y))


# 40 out of 55 coefficients are reduced to 0.
print(len(lasso.coef_))
print(np.sum(lasso.coef_==0))

# Although 55 features were fed into the model, the ridge model only used 15 of them.
# Due to this characteristic,the ridge (or lasso) model can also be used for feature selection purposes.


# Show the name of the coefficients that is greater than 0 in the lass model

lasso_coef = pd.Series(lasso.coef_, poly_5.get_feature_names_out())
lasso_coef[lasso_coef != 0]






from sklearn.linear_model import Ridge

ridge = Ridge()
ridge.fit(train_scaled, train_y)
print(ridge.score(train_scaled, train_y))


print(ridge.score(test_scaled, test_y))


train_score_r = []
test_score_r = []


alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]
# Increase the alpha value in increments of 10 times from 0.001 to 100
# while training the Ridge regression model.

for alpha in alpha_list:
    # ridge model
    ridge = Ridge(alpha=alpha)
    # train ridge model
    ridge.fit(train_scaled, train_y)
    # Save R squares
    train_score_r.append(ridge.score(train_scaled, train_y))
    test_score_r.append(ridge.score(test_scaled, test_y))


plt.plot(np.log10(alpha_list), train_score_r, label='train R2')
plt.plot(np.log10(alpha_list), test_score_r, label='test R2')
#plt.vlines(x=-1, ymin=0.955, ymax=0.999, ls=':', color='r')
plt.legend()
plt.xlabel('$alpha$')
plt.ylabel('$R^2$')
plt.show()


ridge = Ridge() # by default alpha is?
ridge.fit(train_scaled, train_y)

print(ridge.score(train_scaled, train_y))
print(ridge.score(test_scaled, test_y))


# In ridge regression, ALL 55 features are included in the model
print(len(ridge.coef_))
print(np.sum(ridge.coef_==0))


# Show the name of the coefficients that is greater than 0 in the lass model

ridge_coef = pd.Series(ridge.coef_, poly_5.get_feature_names_out())
ridge_coef[ridge_coef != 0]



# fix the code in the cell above to optimize ridge regression by modifying hypterparameter alpha here.

ridge = Ridge(alpha =0.01)
ridge.fit(train_scaled, train_y)

print(ridge.score(train_scaled, train_y))
print(ridge.score(test_scaled, test_y))
