{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kjmobile/lb/blob/main/6_Decision_Tree_Q.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uA_6TRHEMHV"
   },
   "source": [
    "# Decision Tree vs. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdF762MWpLDx"
   },
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VuuF90PHgcgs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "wine = pd.read_csv('https://raw.githubusercontent.com/kjmobile/data/main/ml/wine_csv.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_C_Yy0jXtLi"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2no_OSZyXmZB"
   },
   "outputs": [],
   "source": [
    "wine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dThiku6olKLY"
   },
   "outputs": [],
   "source": [
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFG0edVqrk59"
   },
   "outputs": [],
   "source": [
    "# prints information about a DataFrame\n",
    "wine.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJ9kcxqfiHnw"
   },
   "outputs": [],
   "source": [
    "wine.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ORKbGhMGlQRO"
   },
   "outputs": [],
   "source": [
    "data = wine[['alcohol', 'sugar', 'pH']].to_numpy()\n",
    "target = wine['class'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OMCECWknm3x7"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_input, test_input, train_target, test_target = train_test_split(\n",
    "    data, target, test_size=0.2, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iUJ7AGovnYrm"
   },
   "outputs": [],
   "source": [
    "print(train_input.shape, test_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDoSN0sEnrVc"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(train_input)\n",
    "\n",
    "train_scaled = ss.transform(train_input)\n",
    "test_scaled = ss.transform(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNBO3JgCn7p1"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(train_scaled, train_target)\n",
    "\n",
    "print(lr.score(train_scaled, train_target))\n",
    "print(lr.score(test_scaled, test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x7Epy7XXgaKn"
   },
   "outputs": [],
   "source": [
    "lr.score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Huyjgx02sS1v"
   },
   "source": [
    "### Interpretability of models (logistic regression vs. decision tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nnekb2vbsVxL"
   },
   "outputs": [],
   "source": [
    "print(lr.coef_, lr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfL8p3L5_T-B"
   },
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1yO5owNno9BR"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(criterion='entropy', random_state=17) # what's the default criterion in sklearn?\n",
    "dt.fit(train_scaled, train_target) # what is target variable here?\n",
    "\n",
    "print(dt.score(train_scaled, train_target))\n",
    "print(dt.score(test_scaled, test_target)) # does it show the model is overfitted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ln3bvp_TpBCW"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plot_tree(dt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W5V_uojqTMbn"
   },
   "outputs": [],
   "source": [
    "plot_tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uw9MwzTmRAuN"
   },
   "source": [
    "### Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8U4ER6L97_O"
   },
   "outputs": [],
   "source": [
    "# prune to avoid overfitting.\n",
    "dt = DecisionTreeClassifier(max_depth=4, random_state=17, criterion='entropy')\n",
    "dt.fit(train_scaled, train_target)\n",
    "\n",
    "print(dt.score(train_scaled, train_target))\n",
    "print(dt.score(test_scaled, test_target)) # did this ameliorate overfitting ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "doGWMHOwxpDg"
   },
   "outputs": [],
   "source": [
    "# Why chose max_depth= 4? : Draw a comparison of train vs test scores through a plot by changing max_depth from 3 to 10\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for max_depth in range(3, 11):\n",
    "    dt1 = DecisionTreeClassifier(max_depth=max_depth, random_state=17, criterion='entropy')\n",
    "    dt1.fit(train_scaled, train_target)\n",
    "\n",
    "    train_scores.append(dt1.score(train_scaled, train_target))\n",
    "    test_scores.append(dt1.score(test_scaled, test_target))\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(range(3, 11), train_scores, label=\"train\")\n",
    "plt.plot(range(3, 11), test_scores, label=\"test\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"score\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rFQ9qP4j5DVs"
   },
   "outputs": [],
   "source": [
    "# now plot the tree!\n",
    "plt.figure(figsize=(12,15))\n",
    "plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH']) # when filled=True, does the filled color have any meaning ?\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "odS13_8fymhN",
    "outputId": "a6d61e03-17e9-4c5b-a93d-9061e0b15854"
   },
   "outputs": [],
   "source": [
    "print(dt.feature_importances_) # what does the feature_importance_ show and how, it is used in other modeling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OK4WUR24cPnc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoalxI4YcQsv"
   },
   "source": [
    "Feature importance represents the contribution ratio of each feature to the impurity reduction in a decision tree. The calculation formula is as follows:\n",
    "\n",
    "\n",
    "Feature Importance =\n",
    "$\\frac{\\text{Total Impurity Reduction by the Feature}}{\\text{Sum of Impurity Reductions Across All Features}}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "â€‹\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDAXu9g61MD5"
   },
   "source": [
    "## More to understand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnmB_ayucx5D"
   },
   "source": [
    "### Predict categories using the test_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azAQX2BGe2i2"
   },
   "outputs": [],
   "source": [
    "dt_prediction = dt.predict(test_input)\n",
    "print(dt_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qs64vnvvY3lx"
   },
   "source": [
    "### Show Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z1FCgafAYqN8"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(test_target, dt_prediction)\n",
    "cm_df=pd.DataFrame(cm, columns=['predicted_0', 'predict_1'], index=['actual_0','actual_1'])\n",
    "print(cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7BIdDPsv2AOA"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_7SN9iYfcZX"
   },
   "source": [
    "### Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Bmmuxaa-aRu"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol', 'sugar', 'pH']) # what if you do not pass the argument of feature_names?\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_eGvJYjJv3DN"
   },
   "outputs": [],
   "source": [
    "# In the above chart the inital entropy is .806 and\n",
    "# when the data (5197) is split ( 2954 [on left] + 2243 [on right]), entropy per each is 0.975 and 0.22\n",
    "# Hence the information gain by this split is:\n",
    "\n",
    "initial_entropy = 0.806\n",
    "left_entropy = 0.975\n",
    "right_entropy = 0.22\n",
    "left_proportion = 2954 / (2954 + 2243)\n",
    "right_proportion = 2243 / (2954 + 2243)\n",
    "entropy_after_split = left_proportion * left_entropy + right_proportion * right_entropy\n",
    "information_gain = 0  # fix the line here to compute information gain correctly.\n",
    "print(f'I(Dp) : {initial_entropy}')\n",
    "print(f'I(Dj): {entropy_after_split}')\n",
    "print(f'IG : {information_gain}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_rqSrgS1OZI"
   },
   "source": [
    "### Using min_impurity_decrease instead of max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aH_Ng2X1e_qh"
   },
   "outputs": [],
   "source": [
    "dt2 = DecisionTreeClassifier(min_impurity_decrease=0.001, random_state=17) #min_impurity_decrease= is used to replace what parameter used above?\n",
    "dt2.fit(train_input, train_target)\n",
    "\n",
    "print(dt2.score(train_input, train_target))\n",
    "print(dt2.score(test_input, test_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVBr42JIqg9a"
   },
   "source": [
    "## Entropy vs. Gini Impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2PqphqsqlSq"
   },
   "source": [
    "$$Entropy(D) = -\\sum_{i=1}^{n}p_ilog_2(p_i)$$\n",
    "$$Gini(D) = \\sum_{i=1}^{n}p_i^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTLV_T-cVEi8"
   },
   "source": [
    "Gini impurity and entropy are in a monotonic relationship, meaning they increase or decrease together based on the uniformity or imbalance of the probability distribution.\n",
    "Gini impurity is more commonly used in decision trees than entropy because it has a lower computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rF4MwwUVC4D"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3I2TvyZKnenB"
   },
   "outputs": [],
   "source": [
    "from sympy import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a = symbols('a')\n",
    "b = 1.0 - a\n",
    "gini_impurity_f = 1 - a**2 - b**2\n",
    "epsilon = 1e-10\n",
    "entropy_f = - (a + epsilon) * log(a + epsilon, 2) - (b + epsilon) * log(b + epsilon, 2)\n",
    "\n",
    "# Create the plots individually\n",
    "p1 = plot(gini_impurity_f, (a, 0, 1), label='Gini', show=False,line_color='red')  # Set show=False to prevent display\n",
    "p2 = plot(entropy_f, (a, 0, 1), label='Entropy', show=False, line_color='blue')\n",
    "\n",
    "# Combine the plots\n",
    "p1.append(p2[0])  # Append the second plot's line object to the first plot\n",
    "p1.title =\" Entroy vs. Gini when n = 2\"\n",
    "p1.legend= True\n",
    "p1.show()  # Show the combined plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AAX--DP9rgbv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "gdF762MWpLDx",
    "kfL8p3L5_T-B",
    "ZnmB_ayucx5D",
    "qs64vnvvY3lx",
    "1_7SN9iYfcZX",
    "t_rqSrgS1OZI",
    "IVBr42JIqg9a"
   ],
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
