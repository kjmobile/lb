{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kjmobile/lb/blob/main/8_Ensemble_Learning_Random_Forest_and_Boosting_Q.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zp6fW8MP-mrO"
      },
      "source": [
        "# Ensemble of Decision Trees\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIaIAizcRSG-"
      },
      "source": [
        "## Bagging\n",
        "A method that combines the predictions of multiple independent weak learners to form a more robust model through parallel training and averaging the results.\n",
        "\n",
        " Bootstrapping is used to train each tree on a different data sample, leading to diverse trees that, when combined, reduce variance and improve **generalization**.\n",
        "\n",
        " It also prevents overfitting by learning from only a portion of the data, while the random selection of features at each node reduces correlation between trees, resulting in **less overfitting**."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Random Forest"
      ],
      "metadata": {
        "id": "OcehPL-aKKsL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ioJUlZ0M_uSZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "wine=pd.read_csv('https://raw.githubusercontent.com/kjmobile/data/main/ml/wine_csv.csv')\n",
        "\n",
        "data = wine[['alcohol', 'sugar', 'pH']].to_numpy()\n",
        "target = wine['class'].to_numpy()\n",
        "\n",
        "train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=17)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDKQudr7_8nu",
        "outputId": "f04dab1d-4a55-4c57-d359-ae1216984204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9980758013714472 0.8864707188865033\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(n_jobs=-1, random_state=17)\n",
        "scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1) #default 5-fold cv\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score'])) # some overfitting is shown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rf?"
      ],
      "metadata": {
        "id": "S8ttU2SxhdZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYDbzXNLG8fK",
        "outputId": "451bd738-67b4-4bca-e812-4795ca5bb5e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.22986133 0.50020073 0.26993794]\n"
          ]
        }
      ],
      "source": [
        "rf.fit(train_input, train_target)\n",
        "print(rf.feature_importances_) #sugar is most important"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the 'default' number of trees in random forest?\n",
        "RandomForestClassifier?"
      ],
      "metadata": {
        "id": "VwQv-7XZ3wHR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forests create bootstrapped samples by allowing duplicates from the training set to train decision trees.\n",
        "\n",
        "There are samples that do not get included in these bootstrapped samples, known as **Out-Of-Bag (OOB) samples**.\n",
        "These samples can be used to evaluate the trees that were trained on the bootstrapped samples, acting similarly to a validation set!\n",
        "\n",
        "Using the OOB score can replace the need for cross-validation, allowing for the use of more samples in the training set"
      ],
      "metadata": {
        "id": "usODdDHdC4O0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMc06S1Fa_A-",
        "outputId": "ad5a57f3-0ff3-4bb9-df5e-32ff3ef11d9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8962863190302097\n"
          ]
        }
      ],
      "source": [
        "# Evaluating the performance using oob_score =True instead of using cv.\n",
        "# It allows using more samples for training\n",
        "rf1 = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=17)\n",
        "\n",
        "rf1.fit(train_input, train_target)\n",
        "print(rf1.oob_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When making predictions for classification, the random forest aggregates the predictions of all individual trees. Each tree gives a class probability estimate, and the forest takes the average of these probabilities across all trees for each class (i.e.,soft-voting)"
      ],
      "metadata": {
        "id": "y13j-wz4BuwU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Boosting"
      ],
      "metadata": {
        "id": "BySw8mGIIYCq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A method that sequentially combines multiple weak learners into a strong learner by iteratively adjusting the weights of training instances to focus on difficult cases."
      ],
      "metadata": {
        "id": "tly6DOZTI1bi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adaptive Boosting\n"
      ],
      "metadata": {
        "id": "GHpPCZkCZ9hX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ada = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=500, learning_rate=0.1, random_state=17) #default n_estimator is 50\n",
        "scores = cross_validate(ada, train_input, train_target, return_train_score=True, n_jobs=-1) #default k=5\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "ada.fit(train_input, train_target)\n",
        "print(ada.feature_importances_)\n",
        "\n",
        "#The default base learner in AdaBoost is a DecisionTreeClassifier with max_depth=1, we used a decision stump with slightly less weak.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b0a4e60-e937-4da3-d76d-6aa20d5e14ee",
        "id": "F8eY3TfuaJ92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.888589516133418 0.8703107647886281\n",
            "[0.32478108 0.38463063 0.29058829]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csKxnaxeRX8s"
      },
      "source": [
        "## Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IlNEFkaNsoG",
        "outputId": "1ce8ea3d-8343-4b09-8bb0-1118692d0f6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8881086892152563 0.8720430147331015\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gb = GradientBoostingClassifier(random_state=17)\n",
        "scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "\n",
        "# In gradient boosting, max_depth of the default weak estimator trees are set to 3, Unlike adaBoost (which has max_depth of 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gb?"
      ],
      "metadata": {
        "id": "txwMkbWkqMX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7977a358-fb7a-4c2f-cf2b-88ba3f493eea",
        "id": "B-t0jc3znEpb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9025881343418029 0.8743527430221366\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gb1 = GradientBoostingClassifier(max_depth=4, random_state=17)\n",
        "scores = cross_validate(gb1, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "\n",
        "# method to pass the max_depth is not the same as as adaBost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNpeS8EWpeEi",
        "outputId": "a180a96d-1285-4b63-e444-f1dbd5d2583e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8881086892152563 0.8720430147331015\n"
          ]
        }
      ],
      "source": [
        "gb = GradientBoostingClassifier(random_state=17)\n",
        "scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "#used the default gb model to get the fiture_importances\n",
        "gb.fit(train_input, train_target)\n",
        "print(gb.feature_importances_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<style>\n",
        ".cell {\n",
        "  font-family: \"times new roman\";\n",
        "}\n",
        "\n",
        "\n",
        "Initialize weights for each data point $i$:\n",
        "$$w_i = \\frac{1}{m} \\quad \\text{for} \\quad i = 1, 2, \\ldots, m.$$\n",
        "\n",
        "For the error $ r_j \\text{ of } h_j(x)$:\n",
        "\n",
        "$$r_j = \\frac{\\sum_{i=1}^m w_i \\cdot \\mathbf{1}_{(h_j(x_i) \\neq y_i)}}{\\sum_{i=1}^m w_i}$$\n",
        "\n",
        "\n",
        "For the predictor weight $\\alpha_j$:\n",
        "\n",
        "$$ \\alpha_j = \\eta \\log \\left( \\frac{1 - r_j}{r_j} \\right) $$\n",
        "\n",
        "\n",
        "For updating the weights for each data point $i$:\n",
        "\n",
        "$$\n",
        "w_i \\leftarrow\n",
        "\\begin{cases}\n",
        "w_i & \\text{if } h_j(x_i) = y_i, \\\\\n",
        "w_i \\exp(\\alpha_j) & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "\n",
        "For the final prediction for a new data point $x$:\n",
        "$$ \\hat{y}(x) = \\underset{k}{\\operatorname{argmax}} \\sum_{j=1}^N \\alpha_j \\cdot \\mathbf{1}_{(h_j(x) = k)}$$\n",
        "</style>"
      ],
      "metadata": {
        "id": "njjaop8QKQb7"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}